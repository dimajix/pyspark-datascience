{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "First we load data from HDFS. It is stored as a trivial CSV file with three columns\n",
    "1. product name\n",
    "2. review text\n",
    "3. rating (1 - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema =  StructType([\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('review',StringType(), True),\n",
    "    StructField('rating',StringType(), True),\n",
    "])\n",
    "\n",
    "raw_data = spark.read.schema(schema).csv(\"s3://dimajix-training/data/amazon_baby\")\n",
    "\n",
    "# Print first 5 entries\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Cache Data\n",
    "\n",
    "We need to convert the \"rating\" columns to an integer - but this will obviously fail for the first record, as this one contains the CSV header. So we need to perform some cleanup after trying to convert the data.\n",
    "\n",
    "For helping distributing the workload, we repartition the DataFrame and also cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = raw_data.withColumn('rating',col('rating').cast(IntegerType())) \\\n",
    "    .filter(col('rating').isNotNull()) \\\n",
    "    .filter(col('review').isNotNull()) \\\n",
    "    .repartition(31) \\\n",
    "    .cache()\n",
    "\n",
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train Data / Test Data\n",
    "\n",
    "Now let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = ... # YOUR CODE HERE\n",
    "\n",
    "print(\"train_data: %d\" % train_data.count())\n",
    "print(\"test_data: %d\" % test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Transformer\n",
    "\n",
    "We need a custom Transformer to build the pipeline. The transformer should remove all punctuations from a given column containing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    import string\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c, ' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "class PunctuationCleanupTransformer(Transformer):\n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        \"\"\"\n",
    "        Construction of PunctuationCleanupTransformer which takes two arguments:\n",
    "        inputCol - name of input column\n",
    "        outputCol - name of output column\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Protecetd _transform method which will be called by the public transform\n",
    "        method. You should not call this method directly.\n",
    "        \"\"\"\n",
    "        remove_punctuation_udf = udf(remove_punctuations, StringType())\n",
    "        return dataset.withColumn(self.outputCol, remove_punctuation_udf(self.inputCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Transformer\n",
    "\n",
    "Lets create an instance of the Transformer and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create instance of PunctuationCleanupTransformer and apply it to the data. The result should be stored in clean_data\n",
    "clean_data = ...\n",
    "\n",
    "# Extract a couple of rows, so we can inspect result\n",
    "clean_data.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Transformer for Stemming\n",
    "\n",
    "We need to stem words, and for doing so we use the Python NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_word(words):\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in words]\n",
    "\n",
    "\n",
    "class PorterStemmerTransformer(Transformer):\n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        \"\"\"\n",
    "        Constructor of PorterStemmerTransformer which takes two arguments:\n",
    "        inputCol - name of input column\n",
    "        outputCol - name of output column\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Protecetd _transform method which will be called by the public transform\n",
    "        method. You should not call this method directly.\n",
    "        \"\"\"\n",
    "        stem_word_udf = udf(stem_word, ArrayType(StringType()))\n",
    "        return dataset.withColumn(self.outputCol, stem_word_udf(self.inputCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Transformer\n",
    "\n",
    "Again we want to test the `PorterStemmerTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "\n",
    "# First we need to Tokenize each line. In order to perform this task, we implement the following steps\n",
    "# 1. Instantiate a Tokenizer instance from pyspark.ml.feature\n",
    "# 2. Transform the raw data using the tokenizer\n",
    "tokenizer = Tokenizer(inputCol='review', outputCol='words')\n",
    "tokenized_data = ... # YOUR CODE HERE\n",
    "\n",
    "# Then we can instantiate the PorterStemmerTransformer and use it on the words.\n",
    "stemmer = ... # YOUR CODE HERE\n",
    "stemmed_data = ... # YOUR CODE HERE\n",
    "\n",
    "# Display first 4 entries of the result\n",
    "stemmed_data.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ML Pipeline\n",
    "\n",
    "Now we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before\n",
    "\n",
    "* PunctuationCleanupTransformer - remove punctuations from reviews\n",
    "* Tokenizer - for splitting reviews into words\n",
    "* StopWordRemover - for removing stop words\n",
    "* PorterStemmerTransformer - for stemming words\n",
    "* NGram - for creating NGrams (we'll use two words per n-gram)\n",
    "* CountVectorizer - for creating bag-of-word features from the words\n",
    "* IDF - for creating TF-IDF features from the NGram counts\n",
    "* LogisticRegression - for creating the real model\n",
    "\n",
    "You also need to transform the incoming rating (1-5) to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more SQLTransformer instances. Inside the SQLTransformer instance you simply write SQL code and access the current DataFrame via `__THIS__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "\n",
    "# Define list of stopwords used in StopWordsRemover\n",
    "stopWords=['the','a','and','or', 'it', 'this', 'of', 'an', 'as', 'in', 'on', 'is', 'are', 'to', 'was', 'for', 'then', 'i']\n",
    "stopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "stages = [\n",
    "    # You will probably need in some meaningful order and with appropriate arguments\n",
    "    #   CountVectorizer\n",
    "    #   IDF\n",
    "    #   LogisticRegression\n",
    "    #   NGram\n",
    "    #   PorterStemmerTransformer \n",
    "    #   PunctuationCleanupTransformer\n",
    "    #   SQLTransformer\n",
    "    #   StopWordsRemover\n",
    "    #   Tokenizer\n",
    "]\n",
    "\n",
    "pipe = Pipeline(stages = stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Pipeline Model\n",
    "Using training data, we create a PipelineModel by fitting the Pipeline to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Data\n",
    "\n",
    "Let us do some predictions of the test data using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "pred = ...\n",
    "\n",
    "pred.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "As in the original exercise, we want to use a custom metric for assessing the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "class AccuracyClassificationEvaluator(Evaluator):\n",
    "    def __init__(self, predictionCol='prediction', labelCol='label'):\n",
    "        super(Evaluator,self).__init__()\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "    \n",
    "    def _evaluate(self, dataset):\n",
    "        num_total = dataset.count()\n",
    "        num_correct = dataset.filter(col(self.labelCol) == col(self.predictionCol)).count()\n",
    "        accuracy = float(num_correct) / num_total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Performance\n",
    "\n",
    "With the evaluator we can assess the performance of the prediction and easily compare it to a simple model which always predicts 'positive'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "always_positive = pred.withColumn('prediction',lit(1.0))\n",
    "\n",
    "evaluator = AccuracyClassificationEvaluator(predictionCol='prediction', labelCol='sentiment')\n",
    "\n",
    "print(\"Model Accuracy = %f\" % evaluator.evaluate(pred))\n",
    "print(\"Baseline Accuracy = %f\" % evaluator.evaluate(always_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.1 (Python 3.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
