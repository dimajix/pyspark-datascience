{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Example\n",
    "\n",
    "This example analyzes the bike sharing data set, which contains information of how many bikes have been rented on specific days. In addition to the raw numbers, the data set also contains information about the weather and holidays.\n",
    "\n",
    "Using a linear regression, we try to train a model which can be used to predict the number of bikes rented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from File\n",
    "\n",
    "The data is stored in a CSV file. We first load every column as a String type and then we cast the columns into numeric data types. This approach prevents errors if contents cannot be interpreted as a number. In these cases columns will contain `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('row_id',StringType(),True),\n",
    "    StructField('date',StringType(), True),\n",
    "    StructField('season',StringType(), True),\n",
    "    StructField('year',StringType(), True),\n",
    "    StructField('month',StringType(), True),\n",
    "    StructField('hour',StringType(), True),\n",
    "    StructField('holiday',StringType(), True),\n",
    "    StructField('weekday',StringType(), True),\n",
    "    StructField('workingday',StringType(), True),\n",
    "    StructField('weather',StringType(), True),\n",
    "    StructField('temperature',StringType(), True),\n",
    "    StructField('apparent_temperature',StringType(), True),\n",
    "    StructField('humidity',StringType(), True),\n",
    "    StructField('wind_speed',StringType(), True),\n",
    "    StructField('casual',StringType(), True),\n",
    "    StructField('registered',StringType(), True),\n",
    "    StructField('counter',StringType(), True)\n",
    "    ])\n",
    "\n",
    "raw_data = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .csv('s3://dimajix-training/data/bike-sharing/hour_nohead.csv')\n",
    "\n",
    "data = raw_data.select(\n",
    "    raw_data.row_id.cast('int'),\n",
    "    raw_data.date.cast('string'),\n",
    "    unix_timestamp(raw_data.date, \"yyyy-MM-dd\").alias('ts'),\n",
    "    raw_data.season.cast('double'),\n",
    "    raw_data.year.cast('double'),\n",
    "    raw_data.month.cast('double'),\n",
    "    raw_data.hour.cast('double'),\n",
    "    raw_data.holiday.cast('double'),\n",
    "    raw_data.weekday.cast('double'),\n",
    "    raw_data.workingday.cast('double'),\n",
    "    raw_data.weather.cast('double'),\n",
    "    raw_data.temperature.cast('double'),\n",
    "    raw_data.apparent_temperature.cast('double'),\n",
    "    raw_data.humidity.cast('double'),\n",
    "    raw_data.wind_speed.cast('double'),\n",
    "    raw_data.casual.cast('double'),\n",
    "    raw_data.registered.cast('double'),\n",
    "    raw_data.counter.cast('double')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Data\n",
    "\n",
    "Now that we have loaded the data let's peek inside the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect first 10 entries of the DataFrame\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some Pictures\n",
    "\n",
    "Just t get a rough feeling about the data, we make some pictures of the number of rented bikes against the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import matplotlib.pyplot and also make all plots appear inline in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Plot of Rents per Day\n",
    "The original data contains rents per hour, we want to have the data per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily = data\\\n",
    "    .groupBy('ts').sum('counter') \\\n",
    "    .orderBy('ts')\n",
    "\n",
    "# Convert to Pandas    \n",
    "pdf = daily.toPandas()\n",
    "\n",
    "# Make a Plot\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(pdf['ts'],pdf['sum(counter)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = data \\\n",
    "    .groupBy('ts').sum('casual') \\\n",
    "    .orderBy('ts')\n",
    "    \n",
    "pdf = tmp.toPandas()\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(pdf['ts'],pdf['sum(casual)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = data \\\n",
    "    .groupBy('ts').sum('registered') \\\n",
    "    .orderBy('ts')\n",
    "    \n",
    "pdf = tmp.toPandas()\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(pdf['ts'],pdf['sum(registered)'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use VectorAssembler\n",
    "\n",
    "Most Spark ML methods require one feature column of type `Vector`. In order to generate this feature column from the raw data, Spark provides a `VectorAssembler` which assembles one feature column from arbitrary source columns. The source columns have to be of type `double`.\n",
    "\n",
    "We use it to automatically extract the columns\n",
    "\n",
    "    season, year, month, hour, holiday, weekday, workingday, weather, \n",
    "    temperature, apparent_temperature, humidity, wind_speed\n",
    "    \n",
    "into the new output column 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "\n",
    "tx = VectorAssembler(inputCols=[\n",
    "        'season',\n",
    "        'year',\n",
    "        'month',\n",
    "        'hour',\n",
    "        'holiday',\n",
    "        'weekday',\n",
    "        'workingday',\n",
    "        'weather',\n",
    "        'temperature',\n",
    "        'apparent_temperature',\n",
    "        'humidity',\n",
    "        'wind_speed'],\n",
    "        outputCol='features')\n",
    "td = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Data\n",
    "\n",
    "Since we found an easier way to generate features, we split incoming data first and apply the VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = ... # YOUR CODE HERE\n",
    "\n",
    "# Print sizes of training and testing sets\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Regression\n",
    "\n",
    "1. Apply VectorAssembler\n",
    "2. Perform Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import *\n",
    "\n",
    "asm = # YOUR CODE HERE\n",
    "regression = # YOUR CODE HERE\n",
    "model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Make predictions from test data and print some results. We use the `test_data` DataFrame (which was not used during training). Since this DataFrame does not already contain the feature column, we also need to apply the previously configured `VectorAssembler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create features using the VectorAssembler\n",
    "test_features = # YOUR CODE HERE\n",
    "\n",
    "# Transform the resulting DataFrame using the trained model\n",
    "prediction = # YOUR CODE HERE\n",
    "\n",
    "# Print result\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally lets evaluate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "# Create evaluator instance\n",
    "evaluator = # YOUR CODE HERE\n",
    "\n",
    "# Evaluate predictions\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Adding More Features\n",
    "\n",
    "We might want to add more features in order to improve prediction quality. We propose the following additional features:\n",
    "\n",
    "1. Features for modelling period effects of a year. This can be done by adding the two features:\n",
    "        sin(ts / 31536000 * 6.28318531) \n",
    "        cos(ts / 31536000 * 6.28318531)\n",
    "2. Similarily for modelling periodic effects within a week, the following features can be used:\n",
    "        sin(weekday / 7 * 6.28318531)\n",
    "        cos(weekday / 7 * 6.28318531)\n",
    "3. And for modelling periodic effects within a single day the following features can be used:\n",
    "        sin(hour / 24 * 6.28318531)\n",
    "        cos(hour / 24 * 6.28318531)\n",
    "4. season, one-hot encoded\n",
    "5. weather, one-hot encoded\n",
    "\n",
    "You can use SQLTransformer for arithmetic transformations and a combination of\n",
    "\n",
    "    StringIndexer(inputCol='categoricalFeature', outputCol='categoricalIndex')\n",
    "    OneHotEncoder(inputCol='categoricalIndex', outputCol='categoricalOneHot')\n",
    "    \n",
    "for creating one hot encoded categorical features.\n",
    "\n",
    "We now have a lot of transformations, all of them need to be applied to both the training data and also to the test data. A `Pipeline` can be used to encapsulate multiple feature extraction and model training steps into a single object, which will train also a single pipeline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages= [\n",
    "    SQLTransformer(statement=\"\"\"\n",
    "        SELECT \n",
    "            *,\n",
    "            sin(ts / 31536000 * 6.28318531) as ts_sin,\n",
    "            cos(ts / 31536000 * 6.28318531) ts_cos, \n",
    "            sin(weekday / 7 * 6.28318531) as wd_sin, \n",
    "            cos(weekday / 7 * 6.28318531) as wd_cos,\n",
    "            sin(hour / 24 * 6.28318531) as hour_sin, \n",
    "            cos(hour / 24 * 6.28318531) as hour_cos \n",
    "        FROM __THIS__\n",
    "    \"\"\"),\n",
    "    StringIndexer(inputCol='season',outputCol='iseason'),\n",
    "    OneHotEncoder(inputCol='iseason',outputCol='vseason'),\n",
    "    StringIndexer(inputCol='weather',outputCol='iweather'),\n",
    "    OneHotEncoder(inputCol='iweather',outputCol='vweather'),\n",
    "    VectorAssembler(inputCols=['ts_sin','ts_cos','wd_sin','wd_cos','hour_sin','hour_cos','year','month','hour','holiday','weekday','workingday','temperature','apparent_temperature','humidity','wind_speed','vseason'],outputCol='features'),\n",
    "    LinearRegression(featuresCol='features', labelCol='counter', predictionCol='prediction'),\n",
    "])\n",
    "\n",
    "pipeline_model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the test_data DataFrame using the trained pipeline model\n",
    "prediction = # YOUR CODE HERE\n",
    "\n",
    "# Evlaute pipeline model\n",
    "print(evaluator.evaluate(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.1 (Python 3.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
